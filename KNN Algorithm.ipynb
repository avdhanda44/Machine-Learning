{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "747cb10f",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #FFE4E1; padding: 10px; border: 2px solid #FF1493; border-radius: 5px; box-shadow: 0 0 10px rgba(0, 0, 0, 0.2);\">\n",
    "\n",
    "<h1 style=\"color: #FF69B4; text-align: center; font-size: 28px; font-family: 'Arial', sans-serif;\">Terms related to KNN:</h1>\n",
    "\n",
    "<ol style=\"font-size: 18px;\">\n",
    "  <li style=\"margin-bottom: 10px;\"><u><b>Distance Metric</b></u>: A distance metric measures the similarity or dissimilarity between two data points. Commonly used distance metrics in KNN include Euclidean distance and Manhattan distance. The choice of distance metric depends on the data and problem at hand.</li>\n",
    "  \n",
    "  <li style=\"margin-bottom: 10px;\"><u><b>K Value</b></u>: K is a hyperparameter in KNN that determines the number of neighbors to consider. The appropriate choice of K depends on the dataset and problem. A smaller K value may lead to a more flexible and potentially noisy decision boundary, while a larger K value may lead to a smoother but potentially biased decision boundary.</li>\n",
    "  \n",
    "  <li style=\"margin-bottom: 10px;\"><u><b>Weighted KNN</b></u>: In some cases, it may be beneficial to assign weights to the neighbors based on their distances. This can be achieved by giving more weight to closer neighbors and less weight to farther neighbors. Weighted KNN considers the weighted average of the target values of the neighbors in regression tasks or assigns weights to the class labels in classification tasks.</li>\n",
    "  \n",
    "  <li><u><b>Curse of Dimensionality</b></u>: KNN can suffer from the curse of dimensionality when the number of features or dimensions in the dataset is high. As the number of dimensions increases, the data points become more spread out, making it harder to find meaningful neighbors. Dimensionality reduction techniques or feature selection can be used to mitigate this issue.</li>\n",
    "</ol>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7da8b8d",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #FFE4E1; padding: 10px; border: 2px solid #FF1493; border-radius: 5px; box-shadow: 0 0 10px rgba(0, 0, 0, 0.2);\">\n",
    "\n",
    "<h1 style=\"color: #FF69B4; text-align: center; font-size: 28px; font-family: 'Arial', sans-serif;\">Techniques to Improve the Efficiency and Performance of KNN:</h1>\n",
    "\n",
    "<ol style=\"font-size: 18px;\">\n",
    "  <li style=\"margin-bottom: 10px;\"><u><b>Choosing an Optimal K</b></u>: The choice of K is crucial in KNN. It can be determined through cross-validation or other evaluation techniques. It's important to strike a balance between overfitting (small K) and underfitting (large K).</li>\n",
    "  \n",
    "  <li style=\"margin-bottom: 10px;\"><u><b>Feature Scaling</b></u>: Scaling the input features to a similar range can prevent features with larger magnitudes from dominating the distance calculation.</li>\n",
    "  \n",
    "  <li><u><b>Handling Imbalanced Data</b></u>: If the dataset has imbalanced class distribution, techniques like oversampling or undersampling can be used to ensure equal representation of all classes.</li>\n",
    "</ol>\n",
    "\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
